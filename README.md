
[cc-by-nc-sa]: http://creativecommons.org/licenses/by-nc-sa/4.0/
[cc-by-nc-sa-image]: https://licensebuttons.net/l/by-nc-sa/4.0/88x31.png
[cc-by-nc-sa-shield]: https://img.shields.io/badge/License-CC%20BY--NC--SA%204.0-lightgrey.svg


> [![CC BY-NC-SA 4.0][cc-by-nc-sa-shield]][cc-by-nc-sa] &ensp; [![CC BY-NC-SA 4.0][cc-by-nc-sa-image]][cc-by-nc-sa]
>
> Unless stated otherwise, the datasets, code, and images in this repository are licensed under the [Creative Commons Attributionâ€“NonCommercialâ€“ShareAlike 4.0 International Licence][cc-by-nc-sa]. 
> 
> Copyright (c) 2025 Authors of [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034).
<br>

# The FTS-OBP Method and EduRABSA_SLM Models

This repository provides the code, datasets, and experimental results for our paper,
[Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034).

It includes the following resources related to Aspect-based Sentiment Analysis (ABSA), with a focus on education-domain text analysis:

1) Code implementing our FTS-OBP Evaluation Method for ABSA.

2) Download links to the EduRABSA_SLM Models for education review ABSA tasks.

3) Training parameters, training logs, test outputs, and evaluation results for the EduRABSA_SLM Models.



To cite any of the above content:

```
@misc{hua2025dataefficientadaptationnovelevaluation,
      title={Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis}, 
      author={Yan Cathy Hua and Paul Denny and JÃ¶rg Wicker and Katerina TaÅ¡kova},
      year={2025},
      eprint={2511.03034},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2511.03034}, 
}
``` 

-----------------------

## The FTS-OBP Evaluation Method

**FTS-OBP** (Flexible Text Similarity Matching and Optimal Bipartite Pairing) is our original evaluation method for ABSA, which accommodates realistic extraction boundary variations while maintaining strong correlation with traditional metrics and offering fine-grained diagnostics. Please refer to the paper [Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034) for full details and validation results. 

### More Details

<details>
<summary><strong>Please click to expand</strong></summary>

The implementation of this method is available at [./util/absa_evaluator.py](util/absa_evaluator.py). Our implementation of FTS-OBP takes the model output files as shown in `./OUTPUT_1_final_test_output_files`, and generates fine-grained output at the task, input-entry, and component-pair levels. 


* **Usage Example 1** - A complete example of running inference with an **EduRABSA_SLM** model and evaluating its output using FTS-OBP is available at
 [./scripts/2a_lora_model_inference_and_eval_with_FTS_OBP.ipynb](./scripts/2a_lora_model_inference_and_eval_with_FTS_OBP.ipynb) for using LoRA adaptors,  and script [./scripts/2c_merged_model_inference_and_eval_with_FTS_OBP.ipynb](./scripts/2c_merged_model_inference_and_eval_with_FTS_OBP.ipynb) for using merged models.

 - You can view the corresponding FTS-OBP input file (model output file) generated by the example script in [./OUTPUTS/OUTPUT_1_final_test_output_files/_demo](OUTPUTS/OUTPUT_1_final_test_output_files/_demo),  and the FTS-OBP evaluation output file in [./OUTPUTS/OUTPUT_2a_final_test_output_FTS-0BP_evaluation/_demo](OUTPUTS/OUTPUT_2a_final_test_output_FTS-0BP_evaluation/_demo).


* **Usage Example 2** - An example of directly using FTS-OBP to evaluate model output file  is available at [./scripts/3a_FTS_OBP_evaluate_model_output_file.ipynb](./scripts/3a_FTS_OBP_evaluate_model_output_file.ipynb)

</details>
<br>

-----------------------

## The EduRABSA_SLM Model Family


The "**EduRABSA_SLM**" model family consists of fine-tuned multi-task small LLMs (SLMs) designed for resource-efficient opinion mining on education-domain reviews of courses, teaching staff, and universities (e.g. student course or teaching evaluations, and open-ended survey responses).


The **EduRABSA_SLM** multi-task models can perform opinion mining across the following fine-grained Aspect-based Sentiment Analysis (ABSA) tasks, extracting outputs for and across each review entry as illustrated in the image below:

* Opinion Extraction (OE)
* Aspect-Opinion Pair-Extraction (AOPE)
* Aspect-opinion Categorisation (AOC; ASC with opinion term)
* Aspect-(opinion)-Sentiment Triplet Extraction (ASTE)
* Aspect-(opinion-category)-Sentiment Quadruplet Extraction (ASQE)

&emsp;&emsp;  <img src="https://github.com/yhua219/ftsobp_and_edurabsa_slm/blob/main/.figs/fig_table_1.png?raw=true" alt="ABSA examples" width="850px;">


### Model Info

<details>
<summary><strong>Please click to expand</strong></summary>

* The **EduRABSA_SLM** LoRA adaptors were fine-tuned for two pre-trained models: [Phi4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) (3.8B parameters) and [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) (1.5B parameters), using the [EduRABSA dataset](https://arxiv.org/abs/2508.17008). 
 

* Full details regarding the development and performance of the **EduRABSA_SLM** models can be found in our paper 
[Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis](https://arxiv.org/abs/2511.03034).


* The fine-tuning process used [this example script](https://huggingface.co/microsoft/Phi-4-mini-instruct/blob/main/sample_finetune.py). The training parameters and training logs are available at [./_training_log](_training_log), where each training log folder contains two files:
  - Full training parameters and training log:  `training_summary.txt`.
  - Tensorboard log file: `"events.out.tfevents....."`,


### ðŸ¤— Huggingface Download Link

* The EduRABSA_SLM models (CUDA & ONNX) and LoRA adaptors are available on Huggingface ðŸ¤— in [the EduRABSA SLM collection](https://huggingface.co/collections/yhua219/edurabsa-slm).


### Licence and Copyright

* The "**EduRABSA_SLM**" LoRA adaptors and any merged models derived from them are licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License][cc-by-nc-sa].   


* The two pre-trained base models [Phi4-mini-instruct](https://huggingface.co/microsoft/Phi-4-mini-instruct) and [Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) were used for inference and training LoRA adaptors. No modifications were made to the original models. The original modelsâ€™ licences and copyright notices (where provided) are included in the corresponding subdirectories of this repository.

    
### How to Use

* To merge the LoRA adaptors with the base model, please see [./scripts/1_merge_lora_adaptor_with_base_model.ipynb](scripts/1_merge_lora_adaptor_with_base_model.ipynb).

* To use the LoRA adaptor with or without saving the merged model, please see [./scripts/2a_lora_model_inference_and_eval_with_FTS_OBP.ipynb](scripts/2a_lora_model_inference_and_eval_with_FTS_OBP.ipynb).

* To use an entire model (in this case Huggingface models), please see [./scripts/2b_merged_model_inference.ipynb](./scripts/2b_merged_model_inference.ipynb) or [./scripts/2c_merged_model_inference_and_eval_with_FTS_OBP.ipynb](./scripts/2c_merged_model_inference_and_eval_with_FTS_OBP.ipynb).

</details>
